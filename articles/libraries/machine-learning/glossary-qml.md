---
title: Libreria Quantum Machine Learning
author: alexeib2
ms.author: alexei.bocharov@microsoft.com
ms.date: 2/27/2020
ms.topic: article
uid: microsoft.quantum.libraries.machine-learning.training
ms.openlocfilehash: f9b33a607a892179795d0700ba3080f9a24ab94a
ms.sourcegitcommit: 6ccea4a2006a47569c4e2c2cb37001e132f17476
ms.translationtype: MT
ms.contentlocale: it-IT
ms.lasthandoff: 02/28/2020
ms.locfileid: "77909773"
---
# <a name="quantum-machine-learning-glossary"></a><span data-ttu-id="f545b-102">Glossario Machine Learning Quantum</span><span class="sxs-lookup"><span data-stu-id="f545b-102">Quantum Machine Learning glossary</span></span>

<span data-ttu-id="f545b-103">Il training di un classificatore Quantum incentrato sul circuito è un processo con molte parti mobili che richiedono la stessa quantità (o leggermente maggiore) di calibrazione per valutazione ed errore come training dei classificatori tradizionali.</span><span class="sxs-lookup"><span data-stu-id="f545b-103">Training of a circuit-centric quantum classifier is a process with many moving parts that require the same (or slightly larger) amount of calibration by trial and error as training of traditional classifiers.</span></span> <span data-ttu-id="f545b-104">Qui vengono definiti i concetti e gli ingredienti principali di questo processo di training.</span><span class="sxs-lookup"><span data-stu-id="f545b-104">Here we define the main concepts and ingredients of this training process.</span></span>

## <a name="trainingtesting-schedules"></a><span data-ttu-id="f545b-105">Pianificazioni di training/test</span><span class="sxs-lookup"><span data-stu-id="f545b-105">Training/testing schedules</span></span>

<span data-ttu-id="f545b-106">Nel contesto del training del classificatore una *pianificazione* descrive un subset di campioni di dati in un set di training o di testing globale.</span><span class="sxs-lookup"><span data-stu-id="f545b-106">In the context of classifier training a *schedule* describes a subset of data samples in an overall training or testing set.</span></span> <span data-ttu-id="f545b-107">Una pianificazione viene in genere definita come una raccolta di indici di esempio.</span><span class="sxs-lookup"><span data-stu-id="f545b-107">A schedule is usually defined as a collection of sample indices.</span></span>

## <a name="parameterbias-scores"></a><span data-ttu-id="f545b-108">Punteggi di parametri/distorsioni</span><span class="sxs-lookup"><span data-stu-id="f545b-108">Parameter/bias scores</span></span>

<span data-ttu-id="f545b-109">Dato un vettore di parametro candidato e una polarizzazione del classificatore, il *Punteggio di convalida* viene misurato in relazione a una pianificazione di convalida scelta e viene espressa da un numero di classificazioni errate conteggiate su tutti gli esempi nella pianificazione.</span><span class="sxs-lookup"><span data-stu-id="f545b-109">Given a candidate parameter vector and a classifier bias, their *validation score* is measured relative to a chosen validation schedule S and is expressed by a number of misclassifications counted over all the samples in the schedule S.</span></span>

## <a name="hyperparameters"></a><span data-ttu-id="f545b-110">Iperparametri</span><span class="sxs-lookup"><span data-stu-id="f545b-110">Hyperparameters</span></span>

<span data-ttu-id="f545b-111">Il processo di training del modello è regolato da determinati valori preimpostati detti *iperparametri*:</span><span class="sxs-lookup"><span data-stu-id="f545b-111">The model training process is governed by certain pre-set values called *hyperparameters*:</span></span>

### <a name="learning-rate"></a><span data-ttu-id="f545b-112">Learning rate</span><span class="sxs-lookup"><span data-stu-id="f545b-112">Learning rate</span></span>

<span data-ttu-id="f545b-113">È uno degli iperparametri chiave.</span><span class="sxs-lookup"><span data-stu-id="f545b-113">It is one of the key hyperparameters.</span></span> <span data-ttu-id="f545b-114">Definisce la quantità di stima della sfumatura stocastica corrente che influisca sull'aggiornamento del parametro.</span><span class="sxs-lookup"><span data-stu-id="f545b-114">It defines how much current stochastic gradient estimate impacts the parameter update.</span></span> <span data-ttu-id="f545b-115">La dimensione del Delta di aggiornamento del parametro è proporzionale alla velocità di apprendimento.</span><span class="sxs-lookup"><span data-stu-id="f545b-115">The size of parameter update delta is proportional to the learning rate.</span></span> <span data-ttu-id="f545b-116">I valori di velocità di apprendimento inferiori portano a un'evoluzione più lenta dei parametri e alla convergenza più lenta, ma i valori eccessivamente grandi di LR possono compromettere la convergenza, perché la discesa della sfumatura non viene mai vincolata a un minimo locale specifico.</span><span class="sxs-lookup"><span data-stu-id="f545b-116">Smaller learning rate values lead to slower parameter evolution and slower convergence, but excessively large values of LR may break the convergence altogether as the gradient descent never commits to a particular local minimum.</span></span> <span data-ttu-id="f545b-117">Mentre la velocità di apprendimento è in modo adattivo regolata dall'algoritmo di training in una certa misura, la selezione di un buon valore iniziale è importante.</span><span class="sxs-lookup"><span data-stu-id="f545b-117">While learning rate is adaptively adjusted by the training algorithm to some extent, selecting a good initial value for it is important.</span></span> <span data-ttu-id="f545b-118">Un normale valore iniziale predefinito per la velocità di apprendimento è 0,1.</span><span class="sxs-lookup"><span data-stu-id="f545b-118">A usual default initial value for learning rate is 0.1.</span></span> <span data-ttu-id="f545b-119">La selezione del valore migliore della velocità di apprendimento è un'arte (vedere, ad esempio, la sezione 4,3 di Goodfellow et al., "Deep Learning", MIT Press, 2017).</span><span class="sxs-lookup"><span data-stu-id="f545b-119">Selecting the best value of learning rate is a fine art (see, for example, section 4.3 of Goodfellow et al.,"Deep learning", MIT Press, 2017).</span></span>

### <a name="minibatch-size"></a><span data-ttu-id="f545b-120">Dimensioni Minibatch</span><span class="sxs-lookup"><span data-stu-id="f545b-120">Minibatch size</span></span>

<span data-ttu-id="f545b-121">Definisce il numero di campioni di dati usati per una singola stima della sfumatura stocastica.</span><span class="sxs-lookup"><span data-stu-id="f545b-121">Defines how many data samples is used for a single estimation of stochastic gradient.</span></span> <span data-ttu-id="f545b-122">I valori più grandi delle dimensioni minibatch in genere comportano una convergenza più affidabile e monotona, ma possono potenzialmente rallentare il processo di training, in quanto il costo di una stima di sfumatura è proporzionale alle dimensioni corrispondenza minima.</span><span class="sxs-lookup"><span data-stu-id="f545b-122">Larger values of minibatch size generally lead to more robust and more monotonic convergence but can potentially slow down the training process, as the cost of any one gradient estimation is proportional to the minimatch size.</span></span> <span data-ttu-id="f545b-123">Un valore predefinito usuale per la dimensione minibatch è 10.</span><span class="sxs-lookup"><span data-stu-id="f545b-123">A usual default value for the minibatch size is 10.</span></span>

### <a name="training-epochs-tolerance-gridlocks"></a><span data-ttu-id="f545b-124">Epoche di training, tolleranza, paralisi</span><span class="sxs-lookup"><span data-stu-id="f545b-124">Training epochs, tolerance, gridlocks</span></span>

<span data-ttu-id="f545b-125">"Epoch" significa che un pass-through completo dei dati di training pianificati.</span><span class="sxs-lookup"><span data-stu-id="f545b-125">"Epoch" means one complete pass through the scheduled training data.</span></span>
<span data-ttu-id="f545b-126">Il numero massimo di epoche per un thread di training (vedere di seguito) dovrebbe essere limitato.</span><span class="sxs-lookup"><span data-stu-id="f545b-126">The maximum number of epochs per a training thread (see below) should be capped.</span></span> <span data-ttu-id="f545b-127">Il thread di training viene definito per terminare (con i parametri candidati più noti) quando è stato eseguito il numero massimo di epoche.</span><span class="sxs-lookup"><span data-stu-id="f545b-127">The training thread is defined to terminate (with the best known candidate parameters) when the maximum number of epochs has been executed.</span></span> <span data-ttu-id="f545b-128">Questa formazione, tuttavia, termina in precedenza quando la frequenza di classificazione errata nella pianificazione della convalida scende al di sotto di una tolleranza scelta.</span><span class="sxs-lookup"><span data-stu-id="f545b-128">However such training would terminate earlier when misclassification rate on validation schedule falls below a chosen tolerance.</span></span> <span data-ttu-id="f545b-129">Si supponga, ad esempio, che la tolleranza di classificazione errata sia 0,01 (1%); se nel set di convalida di 2000 campioni vengono visualizzati meno di 20 classificazioni errate, il livello di tolleranza è stato raggiunto.</span><span class="sxs-lookup"><span data-stu-id="f545b-129">Suppose, for example, that misclassification tolerance is 0.01 (1%); if on validation set of 2000 samples we are seeing fewer than 20 misclassifications, then the tolerance level has been achieved.</span></span> <span data-ttu-id="f545b-130">Un thread di training termina anche in modo anomalo se il Punteggio di convalida del modello candidato non mostra alcun miglioramento rispetto a più epoche consecutive (un ingorgo).</span><span class="sxs-lookup"><span data-stu-id="f545b-130">A training thread also terminates prematurely if the validation score of the candidate model has not shown any improvement over several consecutive epochs (a gridlock).</span></span> <span data-ttu-id="f545b-131">La logica per la terminazione dello stato di ingorghi è attualmente hardcoded.</span><span class="sxs-lookup"><span data-stu-id="f545b-131">The logic for the gridlock termination is currently hardcoded.</span></span>

### <a name="measurements-count"></a><span data-ttu-id="f545b-132">Conteggio misure</span><span class="sxs-lookup"><span data-stu-id="f545b-132">Measurements count</span></span>

<span data-ttu-id="f545b-133">La stima dei punteggi di Training/convalida e dei componenti della sfumatura stocastica su un dispositivo Quantum equivale a stimare le sovrapposizioni dello stato del quantum che richiedono più misurazioni delle osservabili appropriate.</span><span class="sxs-lookup"><span data-stu-id="f545b-133">Estimating the training/validation scores and the components of the stochastic gradient on a quantum device amounts to estimating quantum state overlaps that requires multiple measurements of the appropriate observables.</span></span> <span data-ttu-id="f545b-134">Il numero di misure deve essere ridimensionato come $O (1/\ Epsilon ^ 2) $ dove $ \epsilon $ rappresenta l'errore di stima desiderato.</span><span class="sxs-lookup"><span data-stu-id="f545b-134">The number of measurements should scale as $O(1/\epsilon^2)$ where $\epsilon$ is the desired estimation error.</span></span>
<span data-ttu-id="f545b-135">Come regola generale, il conteggio delle misure iniziali potrebbe essere approssimativamente $1/\ mbox {Tolerance} ^ 2 $ (vedere la definizione di tolleranza nel paragrafo precedente).</span><span class="sxs-lookup"><span data-stu-id="f545b-135">As a rule of thumb, the initial measurements count could be approximately $1/\mbox{tolerance}^2$ (see definition of tolerance in the previous paragraph).</span></span> <span data-ttu-id="f545b-136">È necessario rivedere il conteggio delle misure verso l'alto se la discesa della sfumatura risulta troppo irregolare e la convergenza è troppo difficile da raggiungere.</span><span class="sxs-lookup"><span data-stu-id="f545b-136">One would need to revise the measurement count upward if the gradient descent appears to be too erratic and convergence too hard to achieve.</span></span>

### <a name="training-threads"></a><span data-ttu-id="f545b-137">Thread di training</span><span class="sxs-lookup"><span data-stu-id="f545b-137">Training threads</span></span>

<span data-ttu-id="f545b-138">La funzione di probabilità, che è l'utilità di training per il classificatore, è molto raramente convessa, vale a dire che in genere ha una moltitudine di Optima locali nello spazio dei parametri che può variare significativamente in base alla qualità.</span><span class="sxs-lookup"><span data-stu-id="f545b-138">The likelihood function which is the training utility for the classifier is very seldom convex, meaning that it usually has a multitude of local optima in the parameter space that may differ significantly by quality.</span></span> <span data-ttu-id="f545b-139">Poiché il processo SGD può convergere a un solo Optimum specifico, è importante esplorare più vettori di parametri iniziali.</span><span class="sxs-lookup"><span data-stu-id="f545b-139">Since the SGD process can converge to only one specific optimum, it is important to explore multiple starting parameter vectors.</span></span> <span data-ttu-id="f545b-140">Una pratica comune in machine learning consiste nell'inizializzare i vettori di avvio in modo casuale.</span><span class="sxs-lookup"><span data-stu-id="f545b-140">Common practice in machine learning is to initialize such starting vectors randomly.</span></span> <span data-ttu-id="f545b-141">L'API di training Q # accetta una matrice arbitraria di questi vettori iniziali, ma il codice sottostante li Esplora in modo sequenziale.</span><span class="sxs-lookup"><span data-stu-id="f545b-141">The Q# training API accepts an arbitrary array of such starting vectors but the underlying code explores them sequentially.</span></span> <span data-ttu-id="f545b-142">In un computer multicore o in effetti su qualsiasi architettura di elaborazione parallela è consigliabile eseguire diverse chiamate all'API di training Q # in parallelo con diverse inizializzazioni di parametri tra le chiamate.</span><span class="sxs-lookup"><span data-stu-id="f545b-142">On a multicore computer or in fact on any parallel computing architecture it is advisable to perform several calls to Q# training API in parallel with different parameter initializations across the calls.</span></span>

#### <a name="how-to-modify-the-hyperparameters"></a><span data-ttu-id="f545b-143">Come modificare gli iperparametri</span><span class="sxs-lookup"><span data-stu-id="f545b-143">How to modify the hyperparameters</span></span>

<span data-ttu-id="f545b-144">Nella libreria QML, il modo migliore per modificare gli iperparametri consiste nell'eseguire l'override dei valori predefiniti del tipo definito dall'utente [`TrainingOptions`](xref:microsoft.quantum.machinelearning.trainingoptions).</span><span class="sxs-lookup"><span data-stu-id="f545b-144">In the QML library, the best way to modify the hyperparameters is by overriding the default values of the UDT [`TrainingOptions`](xref:microsoft.quantum.machinelearning.trainingoptions).</span></span> <span data-ttu-id="f545b-145">A tale scopo, viene chiamato con la funzione [`DefaultTrainingOptions`](xref:microsoft.quantum.machinelearning.defaulttrainingoptions) e viene applicato l'operatore `w/` per eseguire l'override dei valori predefiniti.</span><span class="sxs-lookup"><span data-stu-id="f545b-145">To do this we call it with the function [`DefaultTrainingOptions`](xref:microsoft.quantum.machinelearning.defaulttrainingoptions) and apply the operator `w/` to override the default values.</span></span> <span data-ttu-id="f545b-146">Ad esempio, per usare le misurazioni 100.000 e una velocità di apprendimento di 0,01:</span><span class="sxs-lookup"><span data-stu-id="f545b-146">For example, to use 100,000 measurements and a learning rate of 0.01:</span></span>
 ```qsharp
let options = DefaultTrainingOptions()
w/ LearningRate <- 0.01
w/ NMeasurements <- 100000;
 ```
