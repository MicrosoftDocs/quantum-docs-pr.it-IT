---
title: Libreria di apprendimento automatico quantistico
description: Informazioni su come usare Machine Learning nei sistemi quantistici
author: alexeib2
ms.author: alexeib
ms.date: 11/22/2019
ms.topic: conceptual
uid: microsoft.quantum.libraries.machine-learning.intro
no-loc:
- Q#
- $$v
ms.openlocfilehash: e2f4a4a63eef40474856426b3b29652b5d3053b2
ms.sourcegitcommit: 71605ea9cc630e84e7ef29027e1f0ea06299747e
ms.translationtype: MT
ms.contentlocale: it-IT
ms.lasthandoff: 01/26/2021
ms.locfileid: "98854024"
---
# <a name="introduction-to-quantum-machine-learning"></a><span data-ttu-id="ebc77-103">Introduzione a Quantum Machine Learning</span><span class="sxs-lookup"><span data-stu-id="ebc77-103">Introduction to Quantum Machine Learning</span></span>

## <a name="framework-and-goals"></a><span data-ttu-id="ebc77-104">Framework e obiettivi</span><span class="sxs-lookup"><span data-stu-id="ebc77-104">Framework and goals</span></span>

<span data-ttu-id="ebc77-105">La codifica quantistica e l'elaborazione delle informazioni è una potente alternativa ai classificatori quantistici di Machine Learning classici.</span><span class="sxs-lookup"><span data-stu-id="ebc77-105">Quantum encoding and processing of information is a powerful alternative to classical machine learning Quantum classifiers.</span></span> <span data-ttu-id="ebc77-106">In particolare, consente di codificare i dati nei registri Quantum che sono concisi rispetto al numero di funzionalità, impiegando sistematicamente il groviglio Quantum come risorsa computazionale e impiegando la misurazione quantistica per l'inferenza della classe.</span><span class="sxs-lookup"><span data-stu-id="ebc77-106">In particular, it allows us to encode data in quantum registers that are concise relative to the number of features, systematically employing quantum entanglement as computational resource and employing quantum measurement for class inference.</span></span>
<span data-ttu-id="ebc77-107">Il classificatore Quantum incentrato sul circuito è una soluzione Quantum relativamente semplice che combina la codifica dei dati con un circuito Quantum rapidamente impigliante/districare seguito dalla misurazione per dedurre le etichette di classe degli esempi di dati.</span><span class="sxs-lookup"><span data-stu-id="ebc77-107">Circuit centric quantum classifier is a relatively simple quantum solution that combines data encoding with a rapidly entangling/disentangling quantum circuit followed by measurement to infer class labels of data samples.</span></span>
<span data-ttu-id="ebc77-108">L'obiettivo è quello di garantire la caratterizzazione classica e l'archiviazione dei circuiti oggetto, nonché la formazione quantistica/classica ibrida dei parametri del circuito anche per spazi di funzioni estremamente grandi.</span><span class="sxs-lookup"><span data-stu-id="ebc77-108">The goal is to ensure classical characterization and storage of subject circuits, as well as hybrid quantum/classical training of the circuit parameters even for extremely large feature spaces.</span></span>

## <a name="classifier-architecture"></a><span data-ttu-id="ebc77-109">Architettura di classificazione</span><span class="sxs-lookup"><span data-stu-id="ebc77-109">Classifier architecture</span></span>

<span data-ttu-id="ebc77-110">La classificazione è un'attività di Machine Learning supervisionata, in cui l'obiettivo è quello di dedurre le etichette delle classi $ \{ y_1, y_2, \ldots, y_d \} $ di determinati esempi di dati.</span><span class="sxs-lookup"><span data-stu-id="ebc77-110">Classification is a supervised machine learning task, where the goal is to infer class labels $\{y_1,y_2,\ldots,y_d\}$ of certain data samples.</span></span> <span data-ttu-id="ebc77-111">Il "set di dati di training" è una raccolta di esempi $ \mathcal{D} = \{ (x, y)} $ con etichette preassegnate note.</span><span class="sxs-lookup"><span data-stu-id="ebc77-111">The "training data set" is a collection of samples $\mathcal{D}=\{(x,y)}$ with known pre-assigned labels.</span></span> <span data-ttu-id="ebc77-112">Qui $x $ è un campione di dati e $y $ è l'etichetta nota denominata "Training label".</span><span class="sxs-lookup"><span data-stu-id="ebc77-112">Here $x$ is a data sample and $y$ is its known label called "training label".</span></span>
<span data-ttu-id="ebc77-113">In modo simile ai metodi tradizionali, la classificazione quantistica è costituita da tre passaggi:</span><span class="sxs-lookup"><span data-stu-id="ebc77-113">Somewhat similar to traditional methods, quantum classification consists of three steps:</span></span>
- <span data-ttu-id="ebc77-114">codifica dei dati</span><span class="sxs-lookup"><span data-stu-id="ebc77-114">data encoding</span></span>
- <span data-ttu-id="ebc77-115">preparazione dello stato di un classificatore</span><span class="sxs-lookup"><span data-stu-id="ebc77-115">preparation of a classifier state</span></span>
- <span data-ttu-id="ebc77-116">misurazione a causa della natura probabilistica della misurazione, questi tre passaggi devono essere ripetuti più volte.</span><span class="sxs-lookup"><span data-stu-id="ebc77-116">measurement Due to the probabilistic nature of the measurement, these three steps must be repeated multiple times.</span></span> <span data-ttu-id="ebc77-117">Sia la codifica che l'elaborazione dello stato di classificazione vengono eseguite tramite *circuiti Quantum*.</span><span class="sxs-lookup"><span data-stu-id="ebc77-117">Both the encoding and the computing of the classifier state are done by means of *quantum circuits*.</span></span> <span data-ttu-id="ebc77-118">Mentre il circuito di codifica è in genere basato sui dati e senza parametri, il circuito di classificazione contiene un set sufficiente di parametri impensabili.</span><span class="sxs-lookup"><span data-stu-id="ebc77-118">While the encoding circuit is usually data-driven and parameter-free, the classifier circuit contains a sufficient set of learnable parameters.</span></span> 

<span data-ttu-id="ebc77-119">Nella soluzione proposta il circuito di classificazione è costituito da rotazioni a qubit singolo e rotazioni controllate da due qubit.</span><span class="sxs-lookup"><span data-stu-id="ebc77-119">In the proposed solution the classifier circuit is composed of single-qubit rotations and two-qubit controlled rotations.</span></span> <span data-ttu-id="ebc77-120">I parametri disponibili qui sono gli angoli di rotazione.</span><span class="sxs-lookup"><span data-stu-id="ebc77-120">The learnable parameters here are the rotation angles.</span></span> <span data-ttu-id="ebc77-121">La rotazione e la rotazione controllata sono note come *universali* per il calcolo quantistico, il che significa che qualsiasi matrice di peso unitario può essere scomposta in un circuito abbastanza lungo costituito da tali controlli.</span><span class="sxs-lookup"><span data-stu-id="ebc77-121">The rotation and controlled rotation gates are known to be *universal* for quantum computation, which means that any unitary weight matrix can be decomposed into a long enough circuit consisting of such gates.</span></span>

<span data-ttu-id="ebc77-122">Nella versione proposta è supportato un solo circuito seguito da una singola stima di frequenza.</span><span class="sxs-lookup"><span data-stu-id="ebc77-122">In the proposed version, only one circuit followed by a single frequency estimation is supported.</span></span>
<span data-ttu-id="ebc77-123">Quindi, la soluzione è un analogo Quantum di una macchina a vettori di supporto con un kernel polinomiale di basso livello.</span><span class="sxs-lookup"><span data-stu-id="ebc77-123">Thus, the solution is a quantum analog of a support vector machine with a low-degree polynomial kernel.</span></span>

![Perceptron multistrato e classificatore incentrato sul circuito](~/media/DLvsQCC.png)

<span data-ttu-id="ebc77-125">Una progettazione di classificazione quantistica semplice può essere confrontata con una soluzione SVM (Support Vector Machine) tradizionale.</span><span class="sxs-lookup"><span data-stu-id="ebc77-125">A simple quantum classifier design can be compared to a traditional support vector machine (SVM) solution.</span></span> <span data-ttu-id="ebc77-126">L'inferenza per un campione di dati $x $ nel caso di SVM viene eseguita usando un formato del kernel ottimale $ \sum \ alpha_j k (x_j, x) $ dove $k $ è una determinata funzione kernel.</span><span class="sxs-lookup"><span data-stu-id="ebc77-126">The inference for a data sample $x$ in case of SVM is done using an optimal kernel form $\sum \alpha_j  k(x_j,x)$ where $k$ is a certain kernel function.</span></span>

<span data-ttu-id="ebc77-127">Al contrario, un classificatore Quantum usa il predittore $p (y │ x, U (\theta)) = 〈 U (\theta) x | M | U (\theta) x 〉 $, che è simile allo spirito ma tecnicamente piuttosto diverso.</span><span class="sxs-lookup"><span data-stu-id="ebc77-127">By contrast, a quantum classifier uses the predictor $p(y│x,U(\theta))=〈U(\theta)x|M|U(\theta)x〉$, which is similar in spirit but technically quite different.</span></span> <span data-ttu-id="ebc77-128">Pertanto, quando viene utilizzata una codifica di ampiezza semplice, $p (y │ x, U (\theta)) $ è un formato quadratico nelle ampiezze di $x $, ma i coefficienti di questo form non vengono più appresi in modo indipendente; sono invece aggregati dagli elementi della matrice del circuito $U (\theta) $, che in genere presenta un numero significativamente inferiore di parametri riconoscibili $ \theta $ rispetto alla dimensione del vettore $x $.</span><span class="sxs-lookup"><span data-stu-id="ebc77-128">Thus, when a straightforward amplitude encoding is used,  $p(y│x,U(\theta))$ is a quadratic form in the amplitudes of $x$, but the coefficients of this form are no longer learned independently; they are instead aggregated from the matrix elements of the circuit $U(\theta)$, which typically has significantly fewer learnable parameters $\theta$ than the dimension of the vector $x$.</span></span> <span data-ttu-id="ebc77-129">Il grado polinomiale di $p (y │ x, U (\theta)) $ nelle funzionalità originali può essere aumentato a $2 ^ l $ usando una codifica del prodotto Quantum su $l $ copie di $x $.</span><span class="sxs-lookup"><span data-stu-id="ebc77-129">The polynomial degree of $p(y│x,U(\theta))$ in the original features can be increased to $2^l$ by using a quantum product encoding on $l$ copies of $x$.</span></span>

<span data-ttu-id="ebc77-130">L'architettura Esplora i circuiti relativamente superficiali, che devono quindi essere *rapidamente coinvolti* per acquisire tutte le correlazioni tra le funzionalità dei dati in tutti gli intervalli.</span><span class="sxs-lookup"><span data-stu-id="ebc77-130">Our architecture explores relatively shallow circuits, which therefore must be *rapidly entangling* in order to capture all the correlations between the data features at all ranges.</span></span> <span data-ttu-id="ebc77-131">Nella figura seguente è illustrato un esempio del componente del circuito più utile.</span><span class="sxs-lookup"><span data-stu-id="ebc77-131">An example of the most useful rapidly entangling circuit component is shown on figure below.</span></span> <span data-ttu-id="ebc77-132">Anche se un circuito con questa geometria è costituito solo da $3 n + 1 $ Gates, la matrice di peso unitaria calcolata garantisce una significativa relazione tra le funzionalità di $2 ^ n $.</span><span class="sxs-lookup"><span data-stu-id="ebc77-132">Even though a circuit with this geometry consists of only $3 n+1$ gates, the unitary weight matrix that it computes ensures significant cross-talk between $2^n$ features.</span></span>

![Interessare rapidamente il circuito Quantum su 5 qubits (con due livelli ciclici).](~/media/5-qubit-qccc.png)

<span data-ttu-id="ebc77-134">Il circuito nell'esempio precedente è costituito da 6 qubit di controllo singolo $ (G_1, \ldots, G_5; G_ {16} ) $ e 10 2-qubits Gates $ (G_6, \ldots, G_ {15} ) $.</span><span class="sxs-lookup"><span data-stu-id="ebc77-134">The circuit in the above example consists of 6 single-qubit gates $(G_1,\ldots,G_5; G_{16})$ and 10 two-qubits gates $(G_6,\ldots,G_{15})$.</span></span> <span data-ttu-id="ebc77-135">Supponendo che ogni controllo sia definito con un parametro riconoscibile, sono disponibili 16 parametri disponibili, mentre la dimensione dello spazio di Hilbert di 5 qubit è 32.</span><span class="sxs-lookup"><span data-stu-id="ebc77-135">Assuming that each of the gates is defined with one learnable parameter we have 16 learnable parameters, while the dimension of the 5-qubit Hilbert space is 32.</span></span> <span data-ttu-id="ebc77-136">Tale geometria del circuito può essere facilmente generalizzata in qualsiasi $n registro $-qubit, quando $n $ è dispari, producendo circuiti con $3 n + 1 $ parametri per lo spazio delle funzionalità $2 ^ n $-dimensional.</span><span class="sxs-lookup"><span data-stu-id="ebc77-136">Such circuit geometry can be easily generalized to any $n$-qubit register, when $n$ is odd, yielding circuits with $3 n+1$ parameters for $2^n$-dimensional feature space.</span></span>

## <a name="classifier-training-as-a-supervised-learning-task"></a><span data-ttu-id="ebc77-137">Training del classificatore come attività di apprendimento supervisionato</span><span class="sxs-lookup"><span data-stu-id="ebc77-137">Classifier training as a supervised learning task</span></span>

<span data-ttu-id="ebc77-138">Il training di un modello di classificazione comporta l'individuazione di valori ottimali dei parametri operativi, in modo da massimizzare la probabilità media di dedurre le etichette di training corrette tra gli esempi di training.</span><span class="sxs-lookup"><span data-stu-id="ebc77-138">Training of a classifier model involves finding optimal values of its operational parameters, such that they maximize the average likelihood of inferring the correct training labels across the training samples.</span></span>
<span data-ttu-id="ebc77-139">In questo caso, ci occupiamo solo della classificazione a due livelli, ad esempio il caso di $d = $2 e solo due classi con le etichette $y _1, y_2 $.</span><span class="sxs-lookup"><span data-stu-id="ebc77-139">Here, we concern ourselves with two level classification only, i.e. the case of $d=2$ and only two classes with the labels $y_1,y_2$.</span></span>

> [!NOTE]
> <span data-ttu-id="ebc77-140">Uno dei principi per generalizzare i metodi a un numero arbitrario di classi consiste nel sostituire qubits con qudits, ad esempio le unità Quantum con $d $ base States e la misurazione bidirezionale con $d misurazione $-Way.</span><span class="sxs-lookup"><span data-stu-id="ebc77-140">A principled way of generalizing our methods to arbitrary number of classes is to replace qubits with qudits, i.e. quantum units with $d$ basis states, and the two-way measurement with $d$-way measurement.</span></span>

### <a name="likelihood-as-the-training-goal"></a><span data-ttu-id="ebc77-141">Probabilità dell'obiettivo di training</span><span class="sxs-lookup"><span data-stu-id="ebc77-141">Likelihood as the training goal</span></span>

<span data-ttu-id="ebc77-142">Considerato un circuito Quantum $U (\theta) $, dove $ \theta $ è un vettore di parametri e indicante la misurazione finale per $M $, la probabilità media dell'inferenza dell'etichetta corretta è $ $ \begin{align} \mathcal{L} (\theta) = \frac {1} {| \mathcal{D} |} \left (\ Sum_ {(x, y_1) \In\mathcal{D}} P (M = y_1 | U (\theta) x) + \ sum_ {(x, y_2) \in\mathcal{D}} P (M = y_2 | U (\theta) x) \right) \end{align} $ $ where $P (M = y | z) $ è la probabilità di misurare $y $ nello stato quantum $z $.</span><span class="sxs-lookup"><span data-stu-id="ebc77-142">Given a learnable quantum circuit $U(\theta)$, where $\theta$ is a vector of parameters, and denoting the final measurement by $M$, the average likelihood of the correct label inference is $$ \begin{align} \mathcal{L}(\theta)=\frac{1}{|\mathcal{D}|} \left( \sum_{(x,y_1)\in\mathcal{D}} P(M=y_1|U(\theta) x) + \sum_{(x,y_2)\in\mathcal{D}} P(M=y_2|U(\theta) x)\right) \end{align} $$ where $P(M=y|z)$ is the probability of measuring $y$ in quantum state $z$.</span></span>
<span data-ttu-id="ebc77-143">In questo caso è sufficiente comprendere che la funzione di probabilità $ \mathcal{L} (\theta) $ è smussata in $ \theta $ e il relativo derivato in qualsiasi $ \ theta_j $ può essere calcolato essenzialmente dallo stesso protocollo Quantum usato per calcolare la funzione di probabilità stessa.</span><span class="sxs-lookup"><span data-stu-id="ebc77-143">Here, it suffices to understand that the likelihood function $\mathcal{L}(\theta)$ is smooth in $\theta$ and its derivative in any $\theta_j$ can be computed by essentially the same quantum protocol as used for computing the likelihood function itself.</span></span> <span data-ttu-id="ebc77-144">In questo modo è possibile ottimizzare $ \mathcal{L} (\theta) $ per Descent gradient.</span><span class="sxs-lookup"><span data-stu-id="ebc77-144">This allows for optimizing the $\mathcal{L}(\theta)$ by gradient descent.</span></span>

### <a name="classifier-bias-and-training-score"></a><span data-ttu-id="ebc77-145">Distorsione del classificatore e Punteggio di training</span><span class="sxs-lookup"><span data-stu-id="ebc77-145">Classifier bias and training score</span></span>

<span data-ttu-id="ebc77-146">Dato alcuni valori intermedi (o finali) dei parametri in $ \theta $, è necessario identificare un singolo valore reale $b $ know come bias di *classificazione* per eseguire l'inferenza.</span><span class="sxs-lookup"><span data-stu-id="ebc77-146">Given some intermediate (or final) values of the parameters in $\theta$, we need to identify a single real value $b$ know as *classifier bias* to do the inference.</span></span> <span data-ttu-id="ebc77-147">La regola di inferenza delle etichette funziona nel modo seguente:</span><span class="sxs-lookup"><span data-stu-id="ebc77-147">The label inference rule works as follows:</span></span> 
- <span data-ttu-id="ebc77-148">A un esempio $x $ viene assegnata un'etichetta $y _2 $ if e solo se $P (M = y_2 | U (\theta) x) + b > $0,5 (RULE1). in caso contrario, viene assegnata l'etichetta $y _1 $)</span><span class="sxs-lookup"><span data-stu-id="ebc77-148">A sample $x$ is assigned label $y_2$ if and only if $P(M=y_2|U(\theta) x) + b > 0.5$  (RULE1) (otherwise it is assigned label $y_1$)</span></span>

<span data-ttu-id="ebc77-149">Ovviamente $b $ deve essere compreso nell'intervallo $ (-0,5, + 0,5) $ per essere significativo.</span><span class="sxs-lookup"><span data-stu-id="ebc77-149">Clearly $b$ must be in the interval $(-0.5,+0.5)$ to be meaningful.</span></span>

<span data-ttu-id="ebc77-150">Un case di training $ (x, y) \in \mathcal{D} $ è considerato una *classificazione errata* in base alla distorsione $b $ se l'etichetta dedotta per $x $ come per Rule1 è effettivamente diversa da $y $.</span><span class="sxs-lookup"><span data-stu-id="ebc77-150">A training case $(x,y) \in \mathcal{D}$ is considered a *misclassification* given the bias $b$ if the label inferred for $x$ as per RULE1 is actually different from $y$.</span></span> <span data-ttu-id="ebc77-151">Il numero complessivo di classificazioni non configurate è il *Punteggio di training* del classificatore data la distorsione $b $.</span><span class="sxs-lookup"><span data-stu-id="ebc77-151">The overall number of misclassifications is the *training score* of the classifier given the bias $b$.</span></span> <span data-ttu-id="ebc77-152">La distorsione di classificazione *ottimale* $b $ riduce al minimo il Punteggio di training.</span><span class="sxs-lookup"><span data-stu-id="ebc77-152">The *optimal* classifier bias $b$ minimizes the training score.</span></span> <span data-ttu-id="ebc77-153">È facile vedere che, date le stime di probabilità pre-calcolate $ \{ P (M = y_2 | U (\theta) x) | (x, \*) \in\mathcal{D} \} $, la distorsione ottimale del classificatore è disponibile tramite ricerca binaria nell'intervallo $ (-0.5, + 0,5) $ facendo al massimo $ \ Log_2 (| \mathcal{D} |) $ Steps.</span><span class="sxs-lookup"><span data-stu-id="ebc77-153">It is easy to see that, given the precomputed probability estimates $\{ P(M=y_2|U(\theta) x) | (x,\*)\in\mathcal{D} \}$, the optimal classifier bias can be found by binary search in interval $(-0.5,+0.5)$ by making at most $\log_2(|\mathcal{D}|)$ steps.</span></span>

### <a name="reference"></a><span data-ttu-id="ebc77-154">Riferimento</span><span class="sxs-lookup"><span data-stu-id="ebc77-154">Reference</span></span>

<span data-ttu-id="ebc77-155">Queste informazioni dovrebbero essere sufficienti per iniziare a giocare con il codice.</span><span class="sxs-lookup"><span data-stu-id="ebc77-155">This information should be enough to start playing with the code.</span></span> <span data-ttu-id="ebc77-156">Tuttavia, per altre informazioni su questo modello, vedere la proposta originale: [ *"classificatori Quantum incentrati sul circuito", Maria Schuld, Alex Bocharov, Krysta Svore e Nathan Wiebe*](https://arxiv.org/abs/1804.00633)</span><span class="sxs-lookup"><span data-stu-id="ebc77-156">However, if you want to learn more about this model, please read the original proposal: [*'Circuit-centric quantum classifiers', Maria Schuld, Alex Bocharov, Krysta Svore and Nathan Wiebe*](https://arxiv.org/abs/1804.00633)</span></span>

<span data-ttu-id="ebc77-157">Oltre all'esempio di codice visualizzato nei passaggi successivi, è anche possibile iniziare a esplorare la classificazione quantistica in [questa esercitazione](https://github.com/microsoft/QuantumKatas/tree/main/tutorials/QuantumClassification)</span><span class="sxs-lookup"><span data-stu-id="ebc77-157">In addition to the code sample you will see in the next steps, you can also start exploring quantum classification in [this tutorial](https://github.com/microsoft/QuantumKatas/tree/main/tutorials/QuantumClassification)</span></span> 
